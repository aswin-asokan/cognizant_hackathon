# --- Install Dependencies ---
!pip install imbalanced-learn

# --- Imports ---
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, GRU, Dense, RepeatVector, TimeDistributed

# --- 1. Load and Prepare Data ---
print("Loading dataset...")
df = pd.read_csv("delivery_data.csv")
df_original = df.copy()

# Fill missing values
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].fillna('Unknown')
    else:
        df[col] = df[col].fillna(0)

# Convert datetimes
df['Creation Datetime'] = pd.to_datetime(df['Creation Datetime'])
df['Delivery Success Datetime'] = pd.to_datetime(df['Delivery Success Datetime'])

# Feature engineering
df['delivery_duration_minutes'] = (df['Delivery Success Datetime'] - df['Creation Datetime']).dt.total_seconds() / 60
df.loc[df['delivery_duration_minutes'] < 0, 'delivery_duration_minutes'] = 0
df['creation_hour'] = df['Creation Datetime'].dt.hour
df['creation_day_of_week'] = df['Creation Datetime'].dt.dayofweek

# Encode categorical
categorical_cols = ['Delivery Success Hub ID', 'pickup hub id', 'Shop Name', 'Rider Name', 'Granular Status']
for col in categorical_cols:
    df[col] = LabelEncoder().fit_transform(df[col].astype(str))

# Features
features = ['Cod Value', 'delivery_duration_minutes', 'creation_hour', 'creation_day_of_week'] + categorical_cols
X = df[features]

# Pseudo labels (fraud = longest delivery times)
duration_threshold = df['delivery_duration_minutes'].quantile(0.50)  # median
y_true = (df['delivery_duration_minutes'] >= duration_threshold).astype(int)

# --- 2. Train/Test Split ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y_true, test_size=0.2, random_state=42, stratify=y_true
)

# --- 3. Oversample Training Data ---
ros = RandomOverSampler(random_state=42)
X_train_res, y_train_res = ros.fit_resample(X_train, y_train)

print("Balanced training class distribution:")
print(pd.Series(y_train_res).value_counts())

# --- 4. Scale & Reshape ---
scaler = StandardScaler()
X_train_res_scaled = scaler.fit_transform(X_train_res)
X_test_scaled = scaler.transform(X_test)

X_train_res_reshaped = X_train_res_scaled.reshape(X_train_res_scaled.shape[0], 1, X_train_res_scaled.shape[1])
X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])

# --- 5. Build GRU Autoencoder ---
timesteps = X_train_res_reshaped.shape[1]
num_features = X_train_res_reshaped.shape[2]

inputs = Input(shape=(timesteps, num_features))
encoder = GRU(64, activation='relu', return_sequences=True)(inputs)
encoder = GRU(32, activation='relu', return_sequences=False)(encoder)
repeater = RepeatVector(timesteps)(encoder)
decoder = GRU(32, activation='relu', return_sequences=True)(repeater)
decoder = GRU(64, activation='relu', return_sequences=True)(decoder)
output = TimeDistributed(Dense(num_features))(decoder)

model = Model(inputs=inputs, outputs=output)
model.compile(optimizer='adam', loss='mae')

print("Training the GRU Autoencoder...")
history = model.fit(X_train_res_reshaped, X_train_res_reshaped,
                    epochs=50, batch_size=64, validation_split=0.1, verbose=1)

# --- 6. Get Anomaly Scores ---
# Train reconstruction error
X_train_pred = model.predict(X_train_res_reshaped)
mae_train = np.mean(np.abs(X_train_pred - X_train_res_reshaped), axis=(1, 2))

# Test reconstruction error
X_test_pred = model.predict(X_test_reshaped)
mae_test = np.mean(np.abs(X_test_pred - X_test_reshaped), axis=(1, 2))

# --- 7. Use 0.50 Quantile Threshold (same as before) ---
threshold = np.quantile(mae_train, 0.50)

y_train_pred = (mae_train >= threshold).astype(int)
y_test_pred = (mae_test >= threshold).astype(int)

# --- 8. Evaluate on Train ---
acc_train = accuracy_score(y_train_res, y_train_pred)
prec_train = precision_score(y_train_res, y_train_pred)
rec_train = recall_score(y_train_res, y_train_pred)
f1_train = f1_score(y_train_res, y_train_pred)
cm_train = confusion_matrix(y_train_res, y_train_pred)

print("\n--- Evaluation Metrics (Balanced Train Data, GRU) ---")
print(f"Accuracy:  {acc_train:.4f}")
print(f"Precision: {prec_train:.4f}")
print(f"Recall:    {rec_train:.4f}")
print(f"F1-Score:  {f1_train:.4f}")
print(f"Threshold used: {threshold:.4f}")
print("\nConfusion Matrix (Train):")
print(cm_train)

# --- 9. Evaluate on Test ---
acc_test = accuracy_score(y_test, y_test_pred)
prec_test = precision_score(y_test, y_test_pred)
rec_test = recall_score(y_test, y_test_pred)
f1_test = f1_score(y_test, y_test_pred)
cm_test = confusion_matrix(y_test, y_test_pred)

print("\n--- Evaluation Metrics (Test Data, GRU) ---")
print(f"Accuracy:  {acc_test:.4f}")
print(f"Precision: {prec_test:.4f}")
print(f"Recall:    {rec_test:.4f}")
print(f"F1-Score:  {f1_test:.4f}")
print("\nConfusion Matrix (Test):")
print(cm_test)

import matplotlib.pyplot as plt
import seaborn as sns

# --- 1. Training vs Validation Loss ---
plt.figure(figsize=(8,5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('MAE Loss')
plt.title('Training vs Validation Loss')
plt.legend()
plt.show()

# --- 2. Distribution of Reconstruction Errors ---
plt.figure(figsize=(8,5))
sns.histplot(mae_train[y_train_res==0], color='green', label='Normal (Train)', kde=True, stat="density", bins=50)
sns.histplot(mae_train[y_train_res==1], color='red', label='Fraud (Train)', kde=True, stat="density", bins=50, alpha=0.6)
plt.axvline(threshold, color='black', linestyle='--', label=f'Threshold={threshold:.4f}')
plt.xlabel("Reconstruction Error (MAE)")
plt.ylabel("Density")
plt.title("Reconstruction Error Distribution (Train)")
plt.legend()
plt.show()

plt.figure(figsize=(8,5))
sns.histplot(mae_test[y_test==0], color='green', label='Normal (Test)', kde=True, stat="density", bins=50)
sns.histplot(mae_test[y_test==1], color='red', label='Fraud (Test)', kde=True, stat="density", bins=50, alpha=0.6)
plt.axvline(threshold, color='black', linestyle='--', label=f'Threshold={threshold:.4f}')
plt.xlabel("Reconstruction Error (MAE)")
plt.ylabel("Density")
plt.title("Reconstruction Error Distribution (Test)")
plt.legend()
plt.show()

